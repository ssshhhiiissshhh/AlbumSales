---
title: "Album Sales 2"
subtitle: Задача 3. Вариант 1
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(PerformanceAnalytics)
library(gvlma)
library(tidyverse)
library(car)
```


```{r}
# установка пакетов
# install.packages("PerformanceAnalytics")
# install.packages("car")
# install.packages("gvlma")
# install.packages("tidyverse")
```

```{r, eval=FALSE}
# подключаем пакеты для использования в текущей сессии
library(PerformanceAnalytics)
library(gvlma)
library(tidyverse)
library(car)
```

```{r}
# считываем данные. передаем первую строку таблицы, как названия переменных
sales <- read.table("Album Sales 2.dat",header=TRUE)

# основные статистические характеристики данных: 
# минимум, максимум, медиану, среднее и квантили для каждой переменной
summary(sales) 
```

Посмотрим на корреляцию переменных и их распределение
```{r}
# из библиотеки PerformanceAnalytics
chart.Correlation(sales)
```

На графике видно, что корреляция между независимыми переменными достаточно низка, а зависимость sales от остальных переменных линейна (монотонна). Так же распределение данных близко к нормальному (лишь у adverts выраженная ассиметрия).
Построим модель

```{r}
# строим модель, взяв все переменные
lmS<-lm(sales ~ adverts + airplay + attract, data = sales)

# детали модели, включая коэффициенты регрессии, их значимость и общую статистику модели
summary(lmS)
```

Получим таблицу коэффициентов βi с их значениями и уровнями значимости (для каждого коэффициента проверяется гипотеза равенства его нулю). P_value при каждой переменной сильно меньше 0.05. Значит, отклоняется гипотеза о равенстве нулю какого-либо коэффициета.

В предпоследней строке выписан коэффициент детерминации R-squared. Он принимает значения от 0 до 1. Это доля дисперсии данных, объяснённая нашей моделью. Чем он больше, тем модель лучше описывает данные. Мы смотрим на Adjusted R-squared, так как это исправленный R^2. Он учитывает количество переменных (а значит, не будет постоянно расти при увелечении переменных). Adjusted R-squared = 0.6595, то есть мы объяснили 65.95% дисперсии.

В последней строке проверяется нулевая гипотеза, что все коэффициенты регрессии кроме Intercept равны нулю. Поскольку p-value: < 2.2e-16 < 0.05, то регрессия статистически значима.

Проверим условия применимости регрессии: независимость данных (точнее нужна нескоррелированность остатков, но если все данные независимы, то остатки тем более нескоррелированы), остатки имеют нулевое среднее для всех предсказанных значений, остатки имеют нормальное распределение, остатки имеют одинаковую дисперсию для всех предсказанных значений (“гомогенность дисперсии” или, что то же самое, “гомоскедастичность”).

Из первого графика мы уже выяснили, что корреляция между переменными не велика.

Посмотрим на графики.

```{r}
plot(lmS)
```

Из графика Residual vs Fitted видно, что дисперсия остатков постоянна при измененении переменных. На графике квантиль-квантиль (Q-Q residuals) видно, что большинство точек лежит на прямой, что соответствует нормальному распределению остатков. 

Можем так же подтвердить это на следующем графике.

```{r}
# из библиотеки car
qqPlot(lmS)
```

За границы доверительного интервала выходит только две точки. Следовательно остатки нормально распределены. 

Проверка гомоскедастичности: график остатков от предсказанных значений (точки на графике должны быть равномерно распределены по вертикали).

```{r}
# из библиотеки car
residualPlot(lmS, type="response")
```

Выполнено условие гомоскедастичности. Так же среднее остатков ровно нулю, что показывает синяя прямая.

Таким образом, все требования к данным выполнены.

Так же, можно проверить эти требования используя тест из библиотеки gvlma.

```{r}
# из библиотеки gvlma
gvlma(lmS)
```

В столбце Decision видим, что все условия выполнены.

Попробуем убрать каждую переменную, чтобы посмотреть, как ведет себя модель (пусть и веских оснований не было).

```{r}
# по очереди удаляем по одной переменной, проверяя, улучшается ли модель (смотрим adjusted R^2)

lmS1<-lm(sales ~ adverts + airplay, data = sales)
summary(lmS1)

lmS2<-lm(sales ~ attract + airplay, data = sales)
summary(lmS2)

lmS3<-lm(sales ~ adverts + attract, data = sales)
summary(lmS3)
```

Замечаем, что удаление любого параметра из модели приводит к уменьшению adjusted R^2. Хоть и удаление attract приводит к небольшому понижению adjusted R^2, показаний на его ненужность у нас нет (p<0.05).

Так же можем подтвердить это с помощью теста ANOVA, а заодно проведем его для каждой вложенной модели.

```{r}
anova(lmS1, lmS)
anova(lmS2, lmS)
anova(lmS3, lmS)
```

Поскольку результат каждого теста значим (p < 0.05), мы заключаем, что переменные adverts, airplay, attract значимо увеличивают предсказательную силу модели, и её нельзя исключить. То есть, модель lmS “лучше” чем её упрощения.

Дополняем исходную таблицу данными о том, как сработала регрессия.
```{r}
# из библиотеки tidyverse
fortify(lmS)
```


**Вывод**

Конечная модель: sales = -26.61 + 0.08\*adverts + 3.37\*airplay + 11.09*attract

Достоверность определяется значением Adjusted R-squared = 0.6595.

Имеем достоверную слабую линейную зависимость продаж от рекламного бюджета (коэффициент регрессии при члене adverts ~0.08. Это означает, что при увеличении данного коэффициента на 1 и при фиксированных остальных зависимая переменная sales увеличится на 0.08 единиц),

хорошую зависимость от прогонов на радио (коэффициент регрессии при члене airplay ~3. Значит, при увеличении данного коэффициента на 1 и при фиксированных остальных зависимая переменная sales увеличится на 3 единицы) 

и сильную зависимость продаж от оценки экспертами (коэффициент регрессии при члене attracts ~11. То есть, при увеличении данного коэффициента на 1 и при фиксированных остальных зависимая переменная sales увеличится на 11 единиц).


